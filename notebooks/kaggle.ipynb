{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive U-net with Pytorch\n",
    "\n",
    "Simple code to run a baseline-like experiment with pytorch that includes:\n",
    "\n",
    " - Our own pytorch dataset class with the CARVANA dataset\n",
    " - Our own u-net like model\n",
    " - Our own loss-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T13:17:32.740696Z",
     "start_time": "2017-08-25T13:17:32.266127Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create pytorch dataset\n",
    "\n",
    "In my case I previously splitted the training images and masks into a validation set with 10% of the training content. If you do nota have this splitting just set `subset='train'` or `subset='test'` when creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T13:17:32.814057Z",
     "start_time": "2017-08-25T13:17:32.741959Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CARVANA(Dataset):\n",
    "    \"\"\"\n",
    "        CARVANA dataset that contains car images as .jpg. Each car has 16 images\n",
    "        taken in different angles and a unique id: id_01.jpg, id_02.jpg, ..., id_16.jpg\n",
    "        The labels are provided as a .gif image that contains the manually cutout mask\n",
    "        for each training image\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, subset=\"train\", transform=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param root: it has to be a path to the folder that contains the dataset folders\n",
    "        :param train: boolean true if you want the train set false for the test one\n",
    "        :param transform: transform the images and labels\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize variables\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.subset = subset\n",
    "        self.data_path, self.labels_path = [], []\n",
    "\n",
    "        def load_images(path):\n",
    "            \"\"\"\n",
    "            returns all the sorted image paths.\n",
    "\n",
    "            :param path:\n",
    "            :return: array with all the paths to the images\n",
    "            \"\"\"\n",
    "            images_dir = [join(path, f) for f in os.listdir(path) if isfile(join(path, f))]\n",
    "            images_dir.sort()\n",
    "\n",
    "            return images_dir\n",
    "\n",
    "        # load the data regarding the subset\n",
    "        if self.subset == \"train\":\n",
    "            self.data_path = load_images(self.root + \"/train\")\n",
    "            self.labels_path = load_images(self.root + \"/train_masks\")\n",
    "        elif self.subset == \"val\":\n",
    "            self.data_path = load_images(self.root + \"/val\")\n",
    "            self.labels_path = load_images(self.root + \"/val_masks\")\n",
    "        elif self.subset == \"test\":\n",
    "            self.data_path = load_images(self.root + \"/test\")\n",
    "            self.labels_path = None\n",
    "        else:\n",
    "            raise RuntimeError('Invalid subset ' + self.subset + ', it must be one of:'\n",
    "                                                                 ' \\'train\\', \\'val\\' or \\'test\\'')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "\n",
    "        :param index:\n",
    "        :return: tuple (img, target) with the input data and its label\n",
    "        \"\"\"\n",
    "\n",
    "        # load image and labels\n",
    "        img = Image.open(self.data_path[index])\n",
    "        target = Image.open(self.labels_path[index]) if not self.subset == 'test' else None\n",
    "\n",
    "        # apply transforms to both\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            target = self.transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that the dataset was correctly created\n",
    "\n",
    "We create a new objetc with our training set. Images and targets are scaled to 256*256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T13:17:33.588153Z",
     "start_time": "2017-08-25T13:17:32.815185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAA5CAYAAAABfFG9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8fb1235080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create datasets\n",
    "train_dataset = CARVANA(root='../data/',\n",
    "                              subset=\"train\",\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.Scale((256,256)),\n",
    "                                  transforms.ToTensor()])\n",
    "                              )\n",
    "\n",
    "# define the dataloader with the previous dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=8,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=1)\n",
    "\n",
    "# define a simple function to show image-labels\n",
    "def im_show(img_list):\n",
    "    \"\"\"\n",
    "    It receives a list of images and plots them together\n",
    "    :param img_list:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    to_PIL = transforms.ToPILImage()\n",
    "    if len(img_list) > 9:\n",
    "        raise Exception(\"len(img_list) must be smaller than 10\") \n",
    "        \n",
    "    for idx, img in enumerate(img_list):\n",
    "        img = np.array(to_PIL(img))\n",
    "        plt.subplot(100 + 10 * len(img_list) + (idx + 1))\n",
    "        fig = plt.imshow(img)\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "img_list = []\n",
    "for i in range(4):\n",
    "    img, label = train_dataset[i]\n",
    "    img_list.append(img)\n",
    "    img_list.append(label)\n",
    "    \n",
    "im_show(img_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the U-net model\n",
    "\n",
    "Let's create a smal u-net like model as a toy example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the convolutional block\n",
    "\n",
    "First we define the convolutional block so we do not need to re-write every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T13:17:33.598430Z",
     "start_time": "2017-08-25T13:17:33.589351Z"
    }
   },
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Define the convolutional - batch norm - relu block to avoid re-writing it\n",
    "    every time\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_size, out_size, kernel_size=3, padding=1, stride=1):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_size, out_size, kernel_size,\n",
    "                              padding=padding, stride=stride)\n",
    "        self.bn = nn.BatchNorm2d(out_size)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Define the network\n",
    "\n",
    "Then we define the network keeping in mind that down_1 output size should match with the scaled output of up_2, while down_2 size should match with the scaled output of the middle convolution of the u-net model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T13:17:33.675086Z",
     "start_time": "2017-08-25T13:17:33.600222Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class small_UNET_256(nn.Module):\n",
    "    \"\"\"\n",
    "    Define UNET model that accepts a 256 input and mostly uses 3x3 kernels\n",
    "    with stride and padding = 1. It reduces the size of the image to 8x8 pixels\n",
    "    ** It might not work if the input 'x' is not a square.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(small_UNET_256, self).__init__()\n",
    "\n",
    "        self.down_1 = nn.Sequential(\n",
    "            conv_block(3, 16),\n",
    "            conv_block(16, 32, stride=2, padding=1))\n",
    "\n",
    "        self.down_2 = nn.Sequential(\n",
    "            conv_block(32, 64),\n",
    "            conv_block(64, 128))\n",
    "\n",
    "        self.middle = conv_block(128, 128, kernel_size=1, padding= 0)\n",
    "\n",
    "        self.up_2 = nn.Sequential(\n",
    "            conv_block(256, 128),\n",
    "            conv_block(128, 32))\n",
    "\n",
    "        self.up_0 = nn.Sequential(\n",
    "            conv_block(64, 64),\n",
    "            conv_block(64, 32))\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            conv_block(32, 16),\n",
    "            conv_block(16, 1, kernel_size=1, padding=0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 256\n",
    "        down1 = self.down_1(x)\n",
    "        out = F.max_pool2d(down1, kernel_size=2, stride=2)\n",
    "\n",
    "        # 64\n",
    "        down2 = self.down_2(out)\n",
    "        out = F.max_pool2d(down2, kernel_size=2, stride=2)\n",
    "\n",
    "        # 8\n",
    "        out = self.middle(out)\n",
    "\n",
    "        # 64\n",
    "        out = F.upsample(out, scale_factor=2)\n",
    "        out = torch.cat([down2, out], 1)\n",
    "        out = self.up_2(out)\n",
    "\n",
    "        # 128\n",
    "        out = F.upsample(out, scale_factor=2)\n",
    "        out = torch.cat([down1, out], 1)\n",
    "        out = self.up_1(out)\n",
    "\n",
    "        # 256\n",
    "        out = F.upsample(out, scale_factor=2)\n",
    "        return self.output(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the loss function\n",
    "\n",
    "Define the Binary Cross Entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T13:17:33.695601Z",
     "start_time": "2017-08-25T13:17:33.676252Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BCELoss2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Code taken from:\n",
    "    https://www.kaggle.com/c/carvana-image-masking-challenge/discussion/37208\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(BCELoss2d, self).__init__()\n",
    "        self.bce_loss = nn.BCELoss(weight, size_average)\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.sigmoid(logits)\n",
    "        probs_flat = probs.view(-1)\n",
    "        targets_flat = targets.view(-1)\n",
    "        return self.bce_loss(probs_flat, targets_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the training function\n",
    "\n",
    "Set the model to train (batch norms, dropouts etc), set a progress bar and start to loop over the data.\n",
    "\n",
    "Get the inputs and targets, compute the loss, propagate gradients and update the progress-bar with the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T13:17:33.720415Z",
     "start_time": "2017-08-25T13:17:33.696727Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the training function\n",
    "def train(train_loader, model, criterion, epoch, num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    # set a progress bar\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, (images, labels) in pbar:\n",
    "        # Convert torch tensor to Variable\n",
    "        images = Variable(images.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "\n",
    "        # compute output\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # measure loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update progress bar status\n",
    "        pbar.set_description('[TRAIN] - EPOCH %d/ %d - BATCH LOSS: %.4f(avg) '\n",
    "                             % (epoch + 1, num_epochs, loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the training loop\n",
    "\n",
    "Create the model, define the loss function and set-up the optimizer. Place everything in a for loop while calling the `train` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T13:24:53.079555Z",
     "start_time": "2017-08-25T13:17:33.721855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4421cd69184168926defc8d3d857af"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ca38142d0e48beb4df68cba7c4f653"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = small_UNET_256().cuda()\n",
    "criterion = BCELoss2d().cuda()\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      weight_decay=1e-4,\n",
    "                      lr=1e-4,\n",
    "                      momentum=0.9,\n",
    "                      nesterov=True)\n",
    "\n",
    "# run the training loop\n",
    "num_epochs = 2\n",
    "for epoch in range(0, num_epochs):\n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, epoch, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features to add\n",
    "\n",
    " - [ ] define the validation function\n",
    " - [ ] add a custom transform operation for data-augmentation\n",
    " - [ ] connect with tensorboard to log the experiments\n",
    " - [ ] store checkpoints while training\n",
    " - [ ] save output as a .csv ready to upload to the competition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
